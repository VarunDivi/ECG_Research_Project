{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # Models ran in venv python 3.9.16 with GPU computing support\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wfdb\n",
    "import ast\n",
    "import ecg_plot\n",
    "print(tf.config.list_physical_devices()) # Verify you have a GPU available. Not required\n",
    "print(tf.test.is_built_with_cuda())\n",
    "print(tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wfdb\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, BatchNormalization, Activation, Dropout\n",
    "from keras.utils import to_categorical\n",
    "sr =100\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    ### Loading raw data into mutable Datframes\n",
    "    ptb = pd.read_csv('../data/ptbxl_database.csv')\n",
    "    def load_raw_data(df, sampling_rate, path):\n",
    "        if(sampling_rate == 100):\n",
    "            data = [wfdb.rdsamp(path+f) for f in df.filename_lr]\n",
    "        else:\n",
    "            data = [wfdb.rdsamp(path+f) for f in df.filename_hr]\n",
    "        data = np.array([signal for signal, meta in data])\n",
    "        return data\n",
    "    \n",
    "    # load and convert annotation data\n",
    "    Y = pd.read_csv('../data/ptbxl_database.csv', index_col='ecg_id')\n",
    "    Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "    # Load raw signal data\n",
    "    X = load_raw_data(Y, sr, '../data/')\n",
    "\n",
    "    # Load scp_statements.csv for diagnostic aggregation\n",
    "    agg_df = pd.read_csv('../data/scp_statements.csv', index_col=0)\n",
    "    agg_df = agg_df[agg_df.diagnostic == 1]\n",
    "\n",
    "    def aggregate_diagnostic(y_dic):\n",
    "        tmp = []\n",
    "        for key in y_dic.keys():\n",
    "            if key in agg_df.index:\n",
    "                tmp.append(agg_df.loc[key].diagnostic_class)\n",
    "        return list(set(tmp))\n",
    "\n",
    "\n",
    "    # Apply diagnostic superclass\n",
    "    Y['diagnostic_superclass'] = Y.scp_codes.apply(aggregate_diagnostic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "with tf.device('/CPU:0'):\n",
    "    # Define the filter parameters\n",
    "    fs = 100  # Sampling frequency (Hz)\n",
    "    lowcut = 0.5  # Lower cutoff frequency (Hz)\n",
    "    highcut = 40.0  # Higher cutoff frequency (Hz)\n",
    "    filter_order = 4  # Filter order\n",
    "\n",
    "    def bandpass(X, fs, lowcut, highcut, filter_order):\n",
    "        # Apply bandpass filter to each channel\n",
    "        filtered_data = np.zeros_like(X)\n",
    "        for i in range(X.shape[2]):\n",
    "            for j in range(X.shape[0]):\n",
    "                b, a = signal.butter(filter_order, [lowcut, highcut], fs=fs, btype='band', output='ba')\n",
    "                filtered_data[j, :, i] = signal.filtfilt(b, a, X[j, :, i])\n",
    "\n",
    "        # Print the shape of the filtered data\n",
    "        return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    def normalize(X, shape = False):\n",
    "        # Compute mean and standard deviation along axis 1 and 2\n",
    "        X_mean = np.mean(X)\n",
    "        X_std = np.std(X)\n",
    "\n",
    "        if(shape):\n",
    "            print(X.shape)\n",
    "        # Normalize data by subtracting mean and dividing by standard deviation\n",
    "        return ((X - X_mean) / X_std)\n",
    "\n",
    "\n",
    "    bld = 0.5\n",
    "    def baseline_drift(X, baseline, range = 0):\n",
    "        bld_range = (baseline-range, baseline+range)\n",
    "        random_shifts = np.random.uniform(bld_range[0], bld_range[1], size = X.shape)\n",
    "        return X + random_shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    def da_apply(X, functions, shape = False):\n",
    "        X_final = X.copy()\n",
    "\n",
    "        # Applys DA Augments in specified order\n",
    "        for func_dict in functions:\n",
    "            func = func_dict['func']\n",
    "            if(func_dict['params'] == None):\n",
    "                X_final = func(X_final)     \n",
    "            else:\n",
    "                params = func_dict['params']\n",
    "                X_final = func(X_final, *params)  \n",
    "\n",
    "        if(shape == True):\n",
    "            print(X_final.shape)\n",
    "\n",
    "        return X_final\n",
    "\n",
    "    #func_dict = [{'func': normalize, 'params': [None]}]\n",
    "\n",
    "    #func_dict = [{'func': baseline_drift, 'params': [0,0.05]}]\n",
    "\n",
    "    func_dict =[{'func': normalize, 'params': [None]},\n",
    "                {'func': baseline_drift, 'params': [0,0.075]}]\n",
    "\n",
    "\n",
    "    # func_dict =[{'func': bandpass, 'params': [sr, 0.5, 15, 3]}, # 100, 0.5, 10, 3. Change the sampling rate as necessary\n",
    "    #             {'func': normalize, 'params': [None]},  # Normalizing across the entire data set instead of by lead\n",
    "    #             {'func': baseline_drift, 'params': [0,0.075]}]  # I found that simulating a 0.075 drift works best\n",
    "\n",
    "    X_final = da_apply(X, func_dict) # If you want to experiment with data shape more, use a smaller data size. Subset X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    # Split data into train and test\n",
    "    test_fold =10\n",
    "    val_fold = 9\n",
    "\n",
    "    def tvt_split(X, Y, val_fold, test_fold, shape = False):\n",
    "        X_train = X[(Y.strat_fold != test_fold) & (Y.strat_fold != val_fold)]\n",
    "        y_train = Y[(Y.strat_fold != test_fold) & (Y.strat_fold != val_fold)].age\n",
    "\n",
    "        X_val = X[Y.strat_fold == val_fold]\n",
    "        y_val = Y[Y.strat_fold == val_fold].age\n",
    "\n",
    "        X_test = X[(Y.strat_fold == test_fold)]\n",
    "        y_test = Y[Y.strat_fold == test_fold].age\n",
    "\n",
    "        y_train = pd.get_dummies(y_train)\n",
    "        y_val = pd.get_dummies(y_val)\n",
    "        y_test = pd.get_dummies(y_test)\n",
    "\n",
    "        y_train = y_train.idxmax(axis = 1).to_numpy()\n",
    "        y_val = y_val.idxmax(axis = 1).to_numpy()\n",
    "        y_test = y_test.idxmax(axis = 1).to_numpy()\n",
    "\n",
    "        rX_train = X_train[(y_train < 89) & (y_train >= 18)] # Additional filtering of patients older than 89 and younger than 18\n",
    "        ry_train = y_train[(y_train < 89) & (y_train >= 18)]\n",
    "\n",
    "        rX_val = X_val[(y_val < 89) & (y_val >= 18)]\n",
    "        ry_val = y_val[(y_val < 89) & (y_val >= 18)]\n",
    "\n",
    "        rX_test = X_test[(y_test < 89) & (y_test >= 18)]\n",
    "        ry_test = y_test[(y_test < 89) & (y_test >= 18)]\n",
    "\n",
    "        if(shape == True):\n",
    "            print((X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape))\n",
    "\n",
    "        return rX_train, ry_train, rX_val, ry_val, rX_test, ry_test\n",
    "\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = tvt_split(X_final, Y, val_fold, test_fold, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_avl(X_data):\n",
    "    num_rows = len(X_data)\n",
    "    \n",
    "    X_result = np.zeros((num_rows, 1000))\n",
    "\n",
    "    for i, outer in enumerate(X_data):\n",
    "        fv = np.zeros(1000)\n",
    "        fv = [inner_list[3] for inner_list in outer]\n",
    "        X_result[i, :len(fv)] = fv  \n",
    "\n",
    "    return X_result\n",
    "\n",
    "def extract_avr(X_data):\n",
    "    num_rows = len(X_data)\n",
    "    \n",
    "    X_result = np.zeros((num_rows, 1000))\n",
    "\n",
    "    for i, outer in enumerate(X_data):\n",
    "        fv = np.zeros(1000)\n",
    "        fv = [inner_list[4] for inner_list in outer]\n",
    "        X_result[i, :len(fv)] = fv  \n",
    "\n",
    "    return X_result\n",
    "X_tr = extract_avl(X_train)\n",
    "\n",
    "X_va = extract_avl(X_val)\n",
    "\n",
    "X_te = extract_avl(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tra = extract_avr(X_train)\n",
    "X_val = extract_avr(X_val)\n",
    "X_te = extract_avr(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error on Validation Set: 14.003559453859296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14.249230757931887"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_tr, y_train)\n",
    "\n",
    "y_pred = model.predict(X_va)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "print(\"Mean Absolute Error on Validation Set:\", mae)\n",
    "\n",
    "y_test_pred = model.predict(X_te)\n",
    "ma =mean_absolute_error(y_test,y_test_pred)\n",
    "ma"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
