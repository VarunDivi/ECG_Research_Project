{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # Models ran in venv python 3.9.16 with GPU computing support\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Dropout, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, UpSampling1D\n",
    "from tensorflow.keras.layers import Conv1DTranspose, Conv1D, Bidirectional, LSTM, MaxPool1D\n",
    "from tensorflow.keras.layers import LeakyReLU, MaxPooling1D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wfdb\n",
    "import ast\n",
    "import ecg_plot\n",
    "import os\n",
    "import scipy.io as sio\n",
    "import xgboost as xgb\n",
    "import glob\n",
    "import imageio\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import time\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    ### Loading raw data into mutable Datframes\n",
    "    ptb = pd.read_csv('../data/ptbxl_database.csv')\n",
    "    def load_raw_data(df, sampling_rate, path):\n",
    "        if(sampling_rate == 100):\n",
    "            data = [wfdb.rdsamp(path+f) for f in df.filename_lr]\n",
    "        else:\n",
    "            data = [wfdb.rdsamp(path+f) for f in df.filename_hr]\n",
    "        data = np.array([signal for signal, meta in data])\n",
    "        return data\n",
    "    \n",
    "    # load and convert annotation data\n",
    "    Y = pd.read_csv('../data/ptbxl_database.csv', index_col='ecg_id')\n",
    "    Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "    # Load raw signal data\n",
    "    X = load_raw_data(Y, sr, '../data/')\n",
    "\n",
    "    # Load scp_statements.csv for diagnostic aggregation\n",
    "    agg_df = pd.read_csv('../data/scp_statements.csv', index_col=0)\n",
    "    agg_df = agg_df[agg_df.diagnostic == 1]\n",
    "\n",
    "    def aggregate_diagnostic(y_dic):\n",
    "        tmp = []\n",
    "        for key in y_dic.keys():\n",
    "            if key in agg_df.index:\n",
    "                tmp.append(agg_df.loc[key].diagnostic_class)\n",
    "        return list(set(tmp))\n",
    "\n",
    "\n",
    "    # Apply diagnostic superclass\n",
    "    Y['diagnostic_superclass'] = Y.scp_codes.apply(aggregate_diagnostic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((17418, 1000, 12), (17418,))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    # Split data into train and test\n",
    "    test_fold =10\n",
    "    val_fold = 9\n",
    "\n",
    "    def tvt_split(X, Y, val_fold, test_fold, shape = False):\n",
    "        X_train = X[(Y.strat_fold != test_fold) & (Y.strat_fold != val_fold)]\n",
    "        y_train = Y[(Y.strat_fold != test_fold) & (Y.strat_fold != val_fold)].age\n",
    "\n",
    "        y_train = pd.get_dummies(y_train)\n",
    "\n",
    "        y_train = y_train.idxmax(axis = 1).to_numpy()\n",
    "\n",
    "        rX_train = X_train[(y_train < 89) & (y_train >= 75)] # Additional filtering of patients older than 89 and younger than 18\n",
    "        ry_train = y_train[(y_train < 89) & (y_train >= 75)]\n",
    "\n",
    "        if(shape == True):\n",
    "            print((X_train.shape, y_train.shape))\n",
    "\n",
    "        return rX_train, ry_train\n",
    "    \n",
    "    X_train, y_train = tvt_split(X, Y, val_fold, test_fold, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator():\n",
    "\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def D_v3(self):\n",
    "        '''\n",
    "        url:https://github.com/MikhailMurashov/ecgGAN\n",
    "        '''\n",
    "\n",
    "        model = Sequential(name='Discriminator_v3')\n",
    "        model.add(Conv1D(filters=32, kernel_size=16, strides=1, padding='same'))\n",
    "        model.add(LeakyReLU())\n",
    "\n",
    "        # model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv1D(filters=64, kernel_size=16, strides=1, padding='same'))\n",
    "        model.add(LeakyReLU())\n",
    "\n",
    "        model.add(MaxPool1D(pool_size=2))\n",
    "\n",
    "        model.add(Conv1D(filters=128, kernel_size=16, strides=1, padding='same'))\n",
    "        model.add(LeakyReLU())\n",
    "\n",
    "        # model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv1D(filters=256, kernel_size=16, strides=1, padding='same'))\n",
    "        model.add(LeakyReLU())\n",
    "\n",
    "        model.add(MaxPool1D(pool_size=2))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1))\n",
    "        \n",
    "        model.summary()\n",
    "        \n",
    "        signal = Input(shape=self.input_shape)\n",
    "        validity = model(signal)\n",
    "\n",
    "        return Model(inputs=signal, outputs=validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, latent_size, input_shape):\n",
    "        self.latent_size = latent_size\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def G_v5(self):\n",
    "        '''\n",
    "        url:https://github.com/MikhailMurashov/ecgGAN\n",
    "        '''\n",
    "\n",
    "        model = Sequential(name='Generator_v1')\n",
    "        model.add(Reshape((self.latent_size, 1)))\n",
    "        model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "\n",
    "        model.add(Conv1D(filters=128, kernel_size=16, strides=1, padding='same'))\n",
    "        model.add(LeakyReLU())\n",
    "    \n",
    "        model.add(Conv1D(filters=64, kernel_size=16, strides=1, padding='same'))\n",
    "        model.add(LeakyReLU())\n",
    "        \n",
    "        model.add(UpSampling1D(2))\n",
    "        \n",
    "        model.add(Conv1D(filters=32, kernel_size=16, strides=1, padding='same'))\n",
    "        model.add(LeakyReLU())\n",
    "        \n",
    "        model.add(Conv1D(filters=16, kernel_size=16, strides=1, padding='same'))\n",
    "        model.add(LeakyReLU())\n",
    "\n",
    "        model.add(Dense(self.input_shape[0]))\n",
    "        model.add(Activation('tanh'))\n",
    "        model.add(Reshape(self.input_shape))\n",
    "\n",
    "        noise = Input(shape=(self.latent_size,))\n",
    "        signal = model(noise)\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return Model(inputs=noise, outputs=signal) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
